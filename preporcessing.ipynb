{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train data into chunks...\n",
      "Splitting val data into chunks...\n",
      "Saved chunked data in finished_files/chunked3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "#chunk file\n",
    "\n",
    "finished_files_dir = \"finished_files\"\n",
    "chunks_dir = os.path.join(finished_files_dir, \"chunked3\")\n",
    "CHUNK_SIZE = 1000  # num examples per chunk, for the chunked data\n",
    "\n",
    "def chunk_file(set_name):\n",
    "    in_file = 'finished_files/%s.bin' % set_name\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk))  # new chunk\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "\n",
    "\n",
    "def chunk_all():\n",
    "    # Make a dir to hold the chunks\n",
    "    if not os.path.isdir(chunks_dir):\n",
    "        os.mkdir(chunks_dir)\n",
    "    # Chunk the data\n",
    "    for set_name in ['train', 'val']:\n",
    "        print(\"Splitting %s data into chunks...\" % set_name)\n",
    "        chunk_file(set_name)\n",
    "    print(\"Saved chunked data in %s\" % chunks_dir)\n",
    "chunk_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write train_data\n",
    "\n",
    "for i in range(0,9):\n",
    "    print(i)\n",
    "    file_object = open(r\"bytecup2018/bytecup.corpus.train.\"+str(i)+\".txt\")\n",
    "    try:\n",
    "        all_the_text = file_object.readlines()\n",
    "        print(len(all_the_text))\n",
    "        #print(all_the_text[0])\n",
    "        #print(all_the_text[1])\n",
    "        #print(json.loads(all_the_text))\n",
    "\n",
    "\n",
    "        for text in all_the_text:\n",
    "            arr = json.loads(text)\n",
    "            #print(len(arr))\n",
    "            train_id = arr['id']\n",
    "            if train_id%10000==0:\n",
    "                print(train_id)\n",
    "            # Write to file\n",
    "            train_file = os.path.join('bytecup2018/train_data', \"%0d.txt\" % int(train_id))\n",
    "\n",
    "\n",
    "            with open(train_file, \"w\") as f:\n",
    "                f.write(arr['content'] + \"\\n\\n\")\n",
    "                f.write('@highlight' + \"\\n\\n\")\n",
    "                f.write(arr['title'])\n",
    "\n",
    "        #for a in arr:\n",
    "         #    print(a)\n",
    "    finally:\n",
    "        file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write test_data\n",
    "\n",
    "file_object = open(r\"bytecup2018/bytecup.corpus.test_set.txt\")\n",
    "try:\n",
    "    all_the_text = file_object.readlines()\n",
    "    print(len(all_the_text))\n",
    "    #print(all_the_text[0])\n",
    "    #print(all_the_text[1])\n",
    "    #print(json.loads(all_the_text))\n",
    "\n",
    "\n",
    "    for text in all_the_text:\n",
    "        arr = json.loads(text)\n",
    "        #print(len(arr))\n",
    "        train_id = arr['id']\n",
    "\n",
    "        # Write to file\n",
    "        train_file = os.path.join('bytecup2018/test_data', \"%0d.txt\" % int(train_id))\n",
    "\n",
    "\n",
    "        with open(train_file, \"w\") as f:\n",
    "            f.write(arr['content'])\n",
    "            #f.write('@highlight' + \"\\n\\n\")\n",
    "            #f.write(arr['title'])\n",
    "\n",
    "    #for a in arr:\n",
    "     #    print(a)\n",
    "finally:\n",
    "    file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stanford CoreNLP Tokenizer\n",
    "\n",
    "stories_dir = \"bytecup2018/test_data\"\n",
    "tokenized_stories_dir = \"bytecup2018/tokenized_test_data\"\n",
    "\n",
    "\"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n",
    "print(\"Preparing to tokenize %s to %s...\" % (stories_dir, tokenized_stories_dir))\n",
    "stories = os.listdir(stories_dir)\n",
    "# make IO list file\n",
    "print(\"Making list of files to tokenize...\")\n",
    "with open(\"mapping.txt\", \"w\") as f:\n",
    "    for s in stories:\n",
    "        f.write(\"%s \\t %s\\n\" % (os.path.join(stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
    "command = ['java', 'edu.stanford.nlp.process.PTBTokenizer', '-ioFileList', '-preserveLines', 'mapping.txt']\n",
    "print(\"Tokenizing %i files in %s and saving in %s...\" % (len(stories), stories_dir, tokenized_stories_dir))\n",
    "subprocess.call(command)\n",
    "print(\"Stanford CoreNLP Tokenizer has finished.\")\n",
    "os.remove(\"mapping.txt\")\n",
    "\n",
    "# Check that the tokenized stories directory contains the same number of files as the original directory\n",
    "num_orig = len(os.listdir(stories_dir))\n",
    "num_tokenized = len(os.listdir(tokenized_stories_dir))\n",
    "if num_orig != num_tokenized:\n",
    "    raise Exception(\n",
    "        \"The tokenized stories directory %s contains %i files, but it should contain the same number as %s (which has %i files). Was there an error during tokenization?\" % (\n",
    "        tokenized_stories_dir, num_tokenized, stories_dir, num_orig))\n",
    "print(\"Successfully finished tokenizing %s to %s.\\n\" % (stories_dir, tokenized_stories_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lea1_2\n",
    "\n",
    "decode_dir ='bytecup2018/test_data'\n",
    "for file in os.listdir(decode_dir):\n",
    "    file_name = decode_dir + \"/\" +file\n",
    "    with open(file_name,\"r\") as f:\n",
    "        decode = f.read()\n",
    "        #print(decode)\n",
    "        with open(\"submit/test_lead1_2/\"+file,\"w\") as wf:\n",
    "            print(file)\n",
    "            if len(decode.split(\".\")[0])>3:\n",
    "                wf.write(decode.split(\".\")[0])\n",
    "            elif len(decode)<3:\n",
    "                wf.write(decode)\n",
    "            else:\n",
    "                wf.write(decode.split(\".\")[0]+\".\"+decode.split(\".\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition write to bin function\n",
    "\n",
    "def read_text_file(text_file):\n",
    "    lines = []\n",
    "    with open(text_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            lines.append(line.strip())\n",
    "    return lines\n",
    "\n",
    "dm_single_close_quote = u'\\u2019'  # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote,\n",
    "              \")\"]  # acceptable ways to end a sentence\n",
    "\n",
    "# We use these to separate the summary sentences in the .bin datafiles\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "def fix_missing_period(line):\n",
    "    \"\"\"Adds a period to a line that is missing a period\"\"\"\n",
    "    if \"@highlight\" in line: return line\n",
    "    if line == \"\": return line\n",
    "    if line[-1] in END_TOKENS: return line\n",
    "    # print line[-1]\n",
    "    return line + \" .\"\n",
    "\n",
    "def get_art_abs(story_file):\n",
    "    lines = read_text_file(story_file)\n",
    "\n",
    "    # Lowercase everything\n",
    "    lines = [line.lower() for line in lines]\n",
    "\n",
    "    # Put periods on the ends of lines that are missing them (this is a problem in the dataset because many image captions don't end in periods; consequently they end up in the body of the article as run-on sentences)\n",
    "    lines = [fix_missing_period(line) for line in lines]\n",
    "\n",
    "    # Separate out article and abstract sentences\n",
    "    article_lines = []\n",
    "    highlights = []\n",
    "    next_is_highlight = False\n",
    "    for idx, line in enumerate(lines):\n",
    "        if line == \"\":\n",
    "            continue  # empty line\n",
    "        elif line.startswith(\"@highlight\"):\n",
    "            next_is_highlight = True\n",
    "        elif next_is_highlight:\n",
    "            highlights.append(line)\n",
    "        else:\n",
    "            article_lines.append(line)\n",
    "\n",
    "    # Make article into a single string\n",
    "    article = ' '.join(article_lines)\n",
    "\n",
    "    # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
    "    abstract = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in highlights])\n",
    "\n",
    "    return article, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to bin\n",
    "\n",
    "root_dir = 'bytecup2018/tokenized_test_data'\n",
    "out_file = 'finished_files/test.bin'\n",
    "i = 0\n",
    "\n",
    "with open(out_file, 'ab') as writer:\n",
    "    for story_file in os.listdir(root_dir):\n",
    "        i = i+1\n",
    "        if i%10000==0:\n",
    "            print(i)\n",
    "        file = os.path.join(\"bytecup2018/train_data\",story_file)\n",
    "        article, abstract = get_art_abs(file)\n",
    "        #print(story_file)\n",
    "        #print(article)\n",
    "        #print(\"-------------\")\n",
    "        #print(abstract)\n",
    "        # Write to tf.Example\n",
    "        tf_example = example_pb2.Example()\n",
    "        tf_example.features.feature['article'].bytes_list.value.extend([article.encode()])\n",
    "        tf_example.features.feature['abstract'].bytes_list.value.extend([abstract.encode()])\n",
    "        tf_example_str = tf_example.SerializeToString()\n",
    "        str_len = len(tf_example_str)\n",
    "        writer.write(struct.pack('q', str_len))\n",
    "        writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "print(\"Finished writing file %s\\n\" % out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk file\n",
    "\n",
    "finished_files_dir = \"finished_files\"\n",
    "chunks_dir = os.path.join(finished_files_dir, \"chunked3\")\n",
    "CHUNK_SIZE = 1000  # num examples per chunk, for the chunked data\n",
    "\n",
    "def chunk_file(set_name):\n",
    "    in_file = 'finished_files/%s.bin' % set_name\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk))  # new chunk\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "\n",
    "\n",
    "def chunk_all():\n",
    "    # Make a dir to hold the chunks\n",
    "    if not os.path.isdir(chunks_dir):\n",
    "        os.mkdir(chunks_dir)\n",
    "    # Chunk the data\n",
    "    for set_name in ['train', 'val']:\n",
    "        print(\"Splitting %s data into chunks...\" % set_name)\n",
    "        chunk_file(set_name)\n",
    "    print(\"Saved chunked data in %s\" % chunks_dir)\n",
    "chunk_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unk\n",
    "\n",
    "# root_dir为要读取文件的根目录\n",
    "decode_dir = \"submit/decoded\"\n",
    "output_dir = \"submit/result\"\n",
    "# 读取批量文件后要写入的文件\n",
    "#with open(\"abstract.txt\", \"w\") as abstract:\n",
    "\n",
    "# 依次读取根目录下的每一个文件\n",
    "for file in os.listdir(decode_dir):\n",
    "    file_name = decode_dir + \"/\" + file\n",
    "    with open(file_name, 'r') as f:\n",
    "        sentence=f.read()\n",
    "        with open(output_dir + \"/\" + file, 'w') as wf:\n",
    "            s = sentence.replace(\"[UNK] \",\"\")\n",
    "            wf.write(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#修改decode檔名\n",
    "\n",
    "# -*- coding:utf-8 -*-\n",
    "import os\n",
    "\n",
    "# root_dir为要读取文件的根目录\n",
    "decode_dir = \"submit/result\"\n",
    "# 读取批量文件后要写入的文件\n",
    "#with open(\"abstract.txt\", \"w\") as abstract:\n",
    "\n",
    "# 依次读取根目录下的每一个文件\n",
    "for file in os.listdir(decode_dir):\n",
    "    file_name = decode_dir + \"/\" + file\n",
    "    file = str(int(file[:6])+1)\n",
    "    refer_name = decode_dir +\"/\"+file+\".txt\"\n",
    "    os.rename(file_name, refer_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
